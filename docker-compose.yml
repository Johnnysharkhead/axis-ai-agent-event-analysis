version: "3.9"

services:
  backend:
    build: ./backend
    container_name: backend
    ports:
      - "${BACKEND_PORT}:5001"
    env_file:
      - .env
    volumes:
      - ./backend:/app
    environment:
      - FLASK_ENV=development
      - FLASK_RUN_PORT=${BACKEND_PORT}
      - FLASK_APP=main.py
      - PYTHONUNBUFFERED=1
      - MQTT_BROKER_HOST=mosquitto
      - OLLAMA_HOST=http://ollama:11434  # Backend talks to this URL
    depends_on:
      - mosquitto

  frontend:
    build: ./frontend
    container_name: axis-frontend
    ports:
      - "${FRONTEND_PORT}:3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - CHOKIDAR_USEPOLLING=true
      - WATCHPACK_POLLING=true
    command: sh -lc "npm install --no-audit --no-fund && npm start"

  mosquitto:
    build:
      context: ./backend
      dockerfile: mosquitto.Dockerfile
    container_name: mqtt_broker
    ports:
      - "${MQTT_BROKER_PORT:-1883}:1883"
    volumes:
      - ./backend/mosquitto.conf:/mosquitto/config/mosquitto.conf:ro
      - ./mosquitto-data:/mosquitto/data
      - ./mosquitto-log:/mosquitto/log

  # --- AI CONTAINER ---
  ollama:
    build: 
      context: ./ai_service
    container_name: ai_agent
    ports:
      - "11434:11434"
    volumes:
      - ollama_storage:/root/.ollama
    profiles:
      - ai  # Makes this container optional
    
    # --- GPU SETTINGS (Disabled by default) ---
    # If you have an NVIDIA GPU, uncomment the 6 lines below:
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

volumes:
  ollama_storage: